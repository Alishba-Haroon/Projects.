{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Emails.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "# Visualize the distribution of spam and non-spam emails\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='spam', data=data)\n",
    "plt.title('Distribution of Spam and Non-Spam Emails')\n",
    "plt.xlabel('Email Type')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Downsample the majority class to balance the dataset\n",
    "ham_emails = data[data['spam'] == 0]\n",
    "spam_emails = data[data['spam'] == 1]\n",
    "ham_emails = ham_emails.sample(n=len(spam_emails), random_state=42)\n",
    "balanced_data = pd.concat([ham_emails, spam_emails]).reset_index(drop=True)\n",
    "\n",
    "# Visualize the distribution of spam and non-spam emails after downsampling\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='spam', data=balanced_data)\n",
    "plt.title('Distribution of Spam and Non-Spam Emails after Downsampling')\n",
    "plt.xlabel('Email Type')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Preprocess the text data\n",
    "balanced_data['text'] = balanced_data['text'].apply(lambda x: x.replace('Subject', ''))\n",
    "balanced_data['text'] = balanced_data['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "balanced_data['text'] = balanced_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stopwords.words('english')]))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(balanced_data['text'], balanced_data['spam'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a tokenizer to split the text data into words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "\n",
    "# Convert the text data into sequences of words\n",
    "train_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "max_length = 100\n",
    "padded_train = pad_sequences(train_sequences, maxlen=max_length)\n",
    "padded_test = pad_sequences(test_sequences, maxlen=max_length)\n",
    "\n",
    "# Create a model to classify the emails\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32, input_length=max_length))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping and learning rate reduction callbacks\n",
    "early_stopping = EarlyStopping(patience=3, monitor='val_accuracy', restore_best_weights=True)\n",
    "learning_rate_reduction = ReduceLROnPlateau(patience=2, monitor='val_loss', factor=0.5, verbose=0)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(padded_train, train_labels, epochs=20, batch_size=32, validation_data=(padded_test, test_labels), callbacks=[early_stopping, learning_rate_reduction])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(padded_test, test_labels)\n",
    "print(f'Test Loss: {test_loss:.3f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.3f}')\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
